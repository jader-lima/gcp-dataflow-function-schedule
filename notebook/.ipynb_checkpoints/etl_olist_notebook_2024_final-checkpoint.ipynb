{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "\n",
    "#beam imports\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.runners.runner import PipelineState\n",
    "\n",
    "# for datetime manipulation\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# # Local Imports\n",
    "# from utils.common_functions import cleandata\n",
    "# from utils.pipeline_funcs import reTupless,ListtoStr,retCustomer,retReview,retProd,retSeller,dict_toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import apache_beam as beam\n",
    "import re\n",
    "import itertools\n",
    "from datetime import datetime, date\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class Etl_olist_template(PipelineOptions):\n",
    "    def ListtoStrNovo(self, values, current_date):    \n",
    "        values = (current_date, values[0], values[7], values[8][0:10], values[9][0:10], values[10][0:10], values[11][0:10],\n",
    "                  values[12][0:10], values[1], values[2], values[3], values[4], values[13], values[14], values[15],\n",
    "                  values[16], values[20][0:10], values[27], values[28], values[21], values[22], values[23], values[24],\n",
    "                  values[25], values[26])\n",
    "        return ','.join(map(str, values))\n",
    "    \n",
    "# def format_Prod(element):\n",
    "#     return f\"{element[0]},{element[1]}\"    \n",
    "    \n",
    "    \n",
    "def ListtoStr_NN(values, current_date):    \n",
    "    return ','.join(f\"{value}\" for value in values)\n",
    "\n",
    "# 0 até 28\n",
    "# Funções auxiliares para processamento de dados\n",
    "def cleandata(element):\n",
    "#    element = re.sub('\\\"|\\'', '', element)\n",
    "    element = re.sub(r\"[\\\"\\'\\(\\)\\[\\]]\", '', element)\n",
    "    return element.split(',')\n",
    "\n",
    "def dict_toList(element):\n",
    "    item = element[0].split(',')\n",
    "    for key, value in element[1].items():\n",
    "        for v in value:\n",
    "            if isinstance(v, dict):\n",
    "                for key, value in v.items():\n",
    "                    item += [str(value[0])]\n",
    "            else:\n",
    "                item += [str(v)]\n",
    "    return item\n",
    "\n",
    "def retReview(element):\n",
    "    if element is not None and len(element) >= 3 and len(element[1]) == 32:\n",
    "        return element[1], element[2]\n",
    "\n",
    "def retSeller(element):\n",
    "    return element[0], element[2], element[3]\n",
    "\n",
    "def retCustomer(element):\n",
    "    return element[0], element[3], element[4]\n",
    "\n",
    "def retProd(element):\n",
    "    return element[0], element[1]\n",
    "\n",
    "def format_Prod(element):\n",
    "    return f\"{element[0]},{element[1]}\"\n",
    "\n",
    "def retOrderItems(element):\n",
    "    return element[0], element[0],element[3], element[4],element[5], element[6], element[7],element[8]    \n",
    "\n",
    "\n",
    "def reTupless(element):\n",
    "    l = list(element[1].keys())\n",
    "    for i in range(len(l) - 1):\n",
    "        item = element[1].get(l[i])\n",
    "        for x in item:\n",
    "            for y in element[1].get(l[-1]):\n",
    "                if isinstance(y, list):\n",
    "                    yield list(x) + list(y)\n",
    "                else:\n",
    "                    yield list(x) + [y]\n",
    "\n",
    "def CompleteCleanDate(element):\n",
    "    date_fields = [3, 4, 5, 6, 7, 17]\n",
    "    for idx in date_fields:\n",
    "        if element[idx] == '':\n",
    "            element[idx] = '2099-12-31'\n",
    "    return element\n",
    "\n",
    "def list_to_dict(element, cols):\n",
    "    \"\"\"\n",
    "    Recebe 2 listas\n",
    "    Retorna 1 dicionário\n",
    "    \"\"\"\n",
    "    return dict(zip(cols, element))\n",
    "\n",
    "# Função principal de execução do pipeline\n",
    "def run_pipeline_local(input_files):\n",
    "    \n",
    "\n",
    "    current_date = str(date.today())\n",
    "\n",
    "    # Criando o pipeline Beam local\n",
    "    etl_olist_template = Etl_olist_template()\n",
    "\n",
    "    p1 = beam.Pipeline(options=PipelineOptions())\n",
    "\n",
    "    current_date = str(date.today())\n",
    "\n",
    "    # Criando o pipeline Beam local\n",
    "    etl_olist_template = Etl_olist_template()\n",
    "\n",
    "    p1 = beam.Pipeline(options=PipelineOptions())\n",
    "    \n",
    "#     output_header='customer_id,customer_city,customer_state,order_id,payment_sequential,payment_type,payment_installments,payment_value'\n",
    "#     output_header=output_header+',order_id,customer_id,order_status,order_purchase_timestamp,order_approved_at,order_delivered_carrier_date'\n",
    "#     output_header=output_header+',order_delivered_customer_date,order_estimated_delivery_date,review_score'\n",
    "#     output_header=output_header+',order_id, product_id, seller_id ,shipping_limit_date, itens_count, price, freight_value'\n",
    "#     output_header=output_header+',seller_id , seller_city,seller_state,product_id,product_category_name'\n",
    "    \n",
    "    output_header = 'order_id,payment_sequential,payment_type,payment_installments,payment_value,order_status,order_purchase_timestamp,order_approved_at,order_delivered_carrier_date,\\\n",
    "    order_delivered_customer_date,order_estimated_delivery_date,review_score,shipping_limit_date,itens_count,price,freight_value,seller_id,seller_city,seller_state,product_id,\\\n",
    "    product_category_name,customer_id,customer_city,customer_state,load_date'\n",
    "    \n",
    "    header = 'campo_0,campo_1,campo_2,campo_3,campo_4,campo_5,campo_6,campo_7,campo_8,campo_9,campo_10,campo_11,campo_12,campo_13,campo_14,campo_15,campo_16,campo_17,campo_18,\\\n",
    "    campo_19,campo_20,campo_21,campo_22,campo_23,campo_24,campo_25,campo_26,campo_27,campo_28'\n",
    "    \n",
    "\n",
    "    cols_itens = [\n",
    "                \"order_id\",\"order_item_id\",\"product_id\",\"seller_id\",\"shipping_limit_date\",\"price\",\"freight_value\"]\n",
    "    \n",
    "    input_itens_orders = ( \n",
    "                p1 \n",
    "                | 'Read itens_orders data' >> beam.io.ReadFromText(input_files['input_itens'],skip_header_lines=1)\n",
    "                | 'Clean data itens_orders' >> beam.Map(cleandata) \n",
    "                | 'dict for itens_orders' >> beam.Map(list_to_dict, cols_itens)    \n",
    "            )\n",
    "\n",
    "\n",
    "    itens_sum_price = (\n",
    "            input_itens_orders  \n",
    "                | 'dict for sum prices' >> beam.Map(lambda element: (element['order_id']+','+element['product_id'] +','+element['seller_id']+','+element['shipping_limit_date'], float(element['price'])) ) \n",
    "                | 'Group sum prices' >> beam.CombinePerKey(sum)\n",
    "    )\n",
    "\n",
    "    itens_count = (\n",
    "            input_itens_orders \n",
    "                | 'dict for count itens' >> beam.Map(lambda element: (element['order_id']+','+element['product_id'] +','+element['seller_id']+','+element['shipping_limit_date'], int(element['order_item_id']))) \n",
    "                | 'count itens' >> beam.combiners.Count.PerKey()\n",
    "    )\n",
    "\n",
    "    freight_value = (\n",
    "            input_itens_orders \n",
    "                | 'dict for sum freight_value' >> beam.Map(lambda element: (element['order_id']+','+element['product_id'] +','+element['seller_id']+','+element['shipping_limit_date'], float(element['freight_value'])) ) \n",
    "                | 'Group sum freight_value' >> beam.CombinePerKey(sum)\n",
    "    )\n",
    "\n",
    "\n",
    "    join1 = ({'itens_count': itens_count, 'itens_sum_price': itens_sum_price} \n",
    "                | 'itens_count + itens_sum_price' >> beam.CoGroupByKey()                                                               \n",
    "    )\n",
    "\n",
    "\n",
    "    join2 = ({'join1': join1, 'freight_value': freight_value} \n",
    "                | 'join1 + freight_value' >> beam.CoGroupByKey()\n",
    "                | 'dict to list ' >> beam.Map(dict_toList) \n",
    "                | 'create dict join2 ' >> beam.Map(lambda element: (element[2], element))\n",
    "    )\n",
    "\n",
    "\n",
    "    sellers = (\n",
    "        p1\n",
    "            | 'Read seller data' >> beam.io.ReadFromText(input_files['input_seller'],skip_header_lines=1)\n",
    "            | 'Clean data seller' >> beam.Map(cleandata) \n",
    "            | 'return seller data except geo id' >> beam.Map(retSeller)\n",
    "            | 'create dict from seller' >> beam.Map(lambda sellers: (sellers[0], sellers))\n",
    "#            | 'Write seller data' >> beam.io.WriteToText('product',file_name_suffix='.csv') \n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    join3 = ({'join2': join2, 'sellers': sellers} \n",
    "                | 'join2 + sellers' >> beam.CoGroupByKey()\n",
    "                | 'dict to list sellers' >> beam.FlatMap(reTupless) \n",
    "                | 'create dict join3 ' >> beam.Map(lambda itens: (itens[1], itens))#verificar essa linha\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    products = (\n",
    "            p1\n",
    "                | 'Read prod data' >> beam.io.ReadFromText(input_files['input_products'],skip_header_lines=1)\n",
    "                | 'Clean data prod' >> beam.Map(cleandata) \n",
    "                | 'return prod data ' >> beam.Map(retProd)\n",
    "                | 'create dict from prod' >> beam.Map(lambda prod: (prod[0], prod))\n",
    "   )\n",
    "\n",
    "\n",
    "\n",
    "    join4 = ({'join3': join3, 'products': products} \n",
    "                | 'join3 + products' >> beam.CoGroupByKey()\n",
    "                | 'dict to list join4' >> beam.FlatMap(reTupless) \n",
    "                | 'Removing undisired caracters after join' >> beam.Map(ListtoStr_NN,current_date)\n",
    "                | 'Clean data and preparing list' >> beam.Map(cleandata) \n",
    "                | 'dict order id key join 4 ' >> beam.Map(lambda itens: (itens[0], itens))\n",
    "\n",
    "    )  \n",
    "\n",
    "\n",
    "        \n",
    "    order = (\n",
    "            p1\n",
    "                | 'Read orders data' >> beam.io.ReadFromText(input_files['input_order'],skip_header_lines=1)\n",
    "                | 'Clean data orders' >> beam.Map(cleandata) \n",
    "                | 'create dict from orders' >> beam.Map(lambda order: (order[0], order))\n",
    "    #            | 'Write seller data' >> beam.io.WriteToText('outputs/dept.txt') \n",
    "    )\n",
    "\n",
    "\n",
    "    reviews = (\n",
    "            p1\n",
    "                | 'Read reviews data' >> beam.io.ReadFromText(input_files['input_reviews'],skip_header_lines=1)\n",
    "                | 'Clean reviews order' >> beam.Map(cleandata) \n",
    "                | 'return order id and review score' >> beam.Map(retReview)\n",
    "                | 'Check reviews only with id' >> beam.Filter(lambda rev: rev is not None )\n",
    "                | 'create dict from reviews' >> beam.Map(lambda reviews: (reviews[0], reviews[1]))         \n",
    "    )\n",
    "\n",
    "    join5 = ({'order': order, 'reviews': reviews} \n",
    "                | 'orders + reviews' >> beam.CoGroupByKey()\n",
    "                | 'dict to list join5' >> beam.FlatMap(reTupless) \n",
    "                | 'create dict join 5 ' >> beam.Map(lambda itens: (itens[0], itens))\n",
    "    )\n",
    "\n",
    "   \n",
    "    payments = (\n",
    "        p1\n",
    "            | 'Read payments data' >> beam.io.ReadFromText(input_files['input_payments'],skip_header_lines=1)\n",
    "            | 'Clean payments order' >> beam.Map(cleandata) \n",
    "            | 'create dict from payments' >> beam.Map(lambda itens: (itens[0], itens))   \n",
    "    )\n",
    "\n",
    "    join6 = ({ 'payments': payments,'join5': join5} \n",
    "                | ' payments + join5 ' >> beam.CoGroupByKey()\n",
    "                | 'dict to list join6' >> beam.FlatMap(reTupless) \n",
    "                | 'create dict join 6' >> beam.Map(lambda itens: (itens[6], itens))\n",
    "    )\n",
    "    \n",
    "    customer = (\n",
    "        p1\n",
    "            | 'Read customer data' >> beam.io.ReadFromText(input_files['input_customer'],skip_header_lines=1)\n",
    "            | 'Clean customer order' >> beam.Map(cleandata) \n",
    "            | 'return customer id ,state and City' >> beam.Map(retCustomer)\n",
    "            | 'create dict from customer' >> beam.Map(lambda itens: (itens[0], itens))        \n",
    "#            | 'Write results customer' >> beam.io.WriteToText('customer',file_name_suffix='.csv')\n",
    "    )\n",
    "\n",
    "    join7 = ({ 'customer': customer, 'join6': join6} \n",
    "            | 'customer + join6' >> beam.CoGroupByKey()\n",
    "            | 'dict to list join7' >> beam.FlatMap(reTupless) \n",
    "            | 'dict order id key join 7  ' >> beam.Map(lambda itens: (itens[8], itens))\n",
    "    )\n",
    "\n",
    "\n",
    "    join8 = ({ 'join7': join7, 'join4': join4} \n",
    "             | 'join7 + join4' >> beam.CoGroupByKey()\n",
    "             | 'dict to list join8' >> beam.FlatMap(reTupless) \n",
    "             | 'List to string' >> beam.Map(ListtoStr_NN,current_date)\n",
    "             | 'Write results join 8' >> beam.io.WriteToText(input_files['output'], file_name_suffix='.csv',header=header)\n",
    "     )\n",
    "    \n",
    "    \n",
    "\n",
    "    # Executando o pipeline localmente\n",
    "    p1.run()\n",
    "\n",
    "\n",
    "#26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jovyan/.local/share/jupyter/runtime/kernel-2e21becd-8722-4547-b907-c011056319a7.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    }
   ],
   "source": [
    " # Definindo os parâmetros de entrada e saída diretamente\n",
    "input_files = {\n",
    "    'input_itens': 'olist_order_items_dataset.csv',\n",
    "    'input_seller': 'olist_sellers_dataset.csv',\n",
    "    'input_products': 'olist_products_dataset.csv',\n",
    "    'input_order': 'olist_orders_dataset.csv',\n",
    "    'input_reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'input_payments': 'olist_order_payments_dataset.csv',\n",
    "    'input_customer': 'olist_customers_dataset.csv',\n",
    "    'output': 'output_nova_todos_novissimo_sorte'\n",
    "}\n",
    "run_pipeline_local(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import apache_beam as beam\n",
    "import re\n",
    "import itertools\n",
    "from datetime import datetime, date\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class Etl_olist_template(PipelineOptions):\n",
    "    def ListtoStrNovo(self, values, current_date):    \n",
    "        values = (current_date, values[0], values[7], values[8][0:10], values[9][0:10], values[10][0:10], values[11][0:10],\n",
    "                  values[12][0:10], values[1], values[2], values[3], values[4], values[13], values[14], values[15],\n",
    "                  values[16], values[20][0:10], values[27], values[28], values[21], values[22], values[23], values[24],\n",
    "                  values[25], values[26])\n",
    "        return ','.join(map(str, values))\n",
    "    \n",
    "# def format_Prod(element):\n",
    "#     return f\"{element[0]},{element[1]}\"    \n",
    "    \n",
    "    \n",
    "def ListtoStr_NN(values):    \n",
    "    return ','.join(f\"{value}\" for value in values)\n",
    "\n",
    "# 0 até 28\n",
    "# Funções auxiliares para processamento de dados\n",
    "def cleandata(element):\n",
    "#    element = re.sub('\\\"|\\'', '', element)\n",
    "    element = re.sub(r\"[\\\"\\'\\(\\)\\[\\]]\", '', element)\n",
    "    return element.split(',')\n",
    "\n",
    "def dict_toList(element):\n",
    "    item = element[0].split(',')\n",
    "    for key, value in element[1].items():\n",
    "        for v in value:\n",
    "            if isinstance(v, dict):\n",
    "                for key, value in v.items():\n",
    "                    item += [str(value[0])]\n",
    "            else:\n",
    "                item += [str(v)]\n",
    "    return item\n",
    "\n",
    "def retReview(element):\n",
    "    if element is not None and len(element) >= 3 and len(element[1]) == 32:\n",
    "        return element[1], element[2]\n",
    "\n",
    "def retSeller(element):\n",
    "    return element[0], element[2], element[3]\n",
    "\n",
    "def retCustomer(element):\n",
    "    return element[0], element[3], element[4]\n",
    "\n",
    "def retProd(element):\n",
    "    return element[0], element[1]\n",
    "\n",
    "def format_Prod(element):\n",
    "    return f\"{element[0]},{element[1]}\"\n",
    "\n",
    "def retOrderItems(element):\n",
    "    return element[0], element[0],element[3], element[4],element[5], element[6], element[7],element[8]\n",
    "\n",
    "def returnFieldsJoin7Join4(element,current_date):\n",
    "    return element[3],element[4],element[5],element[6],element[7],element[10] ,element[11],element[12],element[13],element[14],element[15],element[16], element[20], element[21], element[22], element[23], element[24], element[25], element[26], element[27], element[28], element[0], element[1], element[2], current_date\n",
    "\n",
    "\n",
    "def reTupless(element):\n",
    "    l = list(element[1].keys())\n",
    "    for i in range(len(l) - 1):\n",
    "        item = element[1].get(l[i])\n",
    "        for x in item:\n",
    "            for y in element[1].get(l[-1]):\n",
    "                if isinstance(y, list):\n",
    "                    yield list(x) + list(y)\n",
    "                else:\n",
    "                    yield list(x) + [y]\n",
    "\n",
    "def CompleteCleanDate(element):\n",
    "    date_fields = [3, 4, 5, 6, 7, 17]\n",
    "    for idx in date_fields:\n",
    "        if element[idx] == '':\n",
    "            element[idx] = '2099-12-31'\n",
    "    return element\n",
    "\n",
    "def list_to_dict(element, cols):\n",
    "    \"\"\"\n",
    "    Recebe 2 listas\n",
    "    Retorna 1 dicionário\n",
    "    \"\"\"\n",
    "    return dict(zip(cols, element))\n",
    "\n",
    "# Função principal de execução do pipeline\n",
    "def run_pipeline_local(input_files):\n",
    "    \n",
    "\n",
    "    current_date = str(date.today())\n",
    "\n",
    "    # Criando o pipeline Beam local\n",
    "    etl_olist_template = Etl_olist_template()\n",
    "\n",
    "    p1 = beam.Pipeline(options=PipelineOptions())\n",
    "\n",
    "    current_date = str(date.today())\n",
    "\n",
    "    # Criando o pipeline Beam local\n",
    "    etl_olist_template = Etl_olist_template()\n",
    "\n",
    "    p1 = beam.Pipeline(options=PipelineOptions())\n",
    "    \n",
    "#     output_header='customer_id,customer_city,customer_state,order_id,payment_sequential,payment_type,payment_installments,payment_value'\n",
    "#     output_header=output_header+',order_id,customer_id,order_status,order_purchase_timestamp,order_approved_at,order_delivered_carrier_date'\n",
    "#     output_header=output_header+',order_delivered_customer_date,order_estimated_delivery_date,review_score'\n",
    "#     output_header=output_header+',order_id, product_id, seller_id ,shipping_limit_date, itens_count, price, freight_value'\n",
    "#     output_header=output_header+',seller_id , seller_city,seller_state,product_id,product_category_name'\n",
    "    \n",
    "    output_header = 'order_id,payment_sequential,payment_type,payment_installments,payment_value,order_status,order_purchase_timestamp,order_approved_at,order_delivered_carrier_date,\\\n",
    "    order_delivered_customer_date,order_estimated_delivery_date,review_score,shipping_limit_date,itens_count,price,freight_value,seller_id,seller_city,seller_state,product_id,\\\n",
    "    product_category_name,customer_id,customer_city,customer_state,load_date'\n",
    "    \n",
    "    header = 'campo_0,campo_1,campo_2,campo_3,campo_4,campo_5,campo_6,campo_7,campo_8,campo_9,campo_10,campo_11,campo_12,campo_13,campo_14,campo_15,campo_16,campo_17,campo_18,\\\n",
    "    campo_19,campo_20,campo_21,campo_22,campo_23,campo_24,campo_25,campo_26,campo_27,campo_28'\n",
    "    \n",
    "\n",
    "    cols_itens = [\n",
    "                \"order_id\",\"order_item_id\",\"product_id\",\"seller_id\",\"shipping_limit_date\",\"price\",\"freight_value\"]\n",
    "    \n",
    "    input_itens_orders = ( \n",
    "                p1 \n",
    "                | 'Read itens_orders data' >> beam.io.ReadFromText(input_files['input_itens'],skip_header_lines=1)\n",
    "                | 'Clean data itens_orders' >> beam.Map(cleandata) \n",
    "                | 'dict for itens_orders' >> beam.Map(list_to_dict, cols_itens)    \n",
    "            )\n",
    "\n",
    "\n",
    "    itens_sum_price = (\n",
    "            input_itens_orders  \n",
    "                | 'dict for sum prices' >> beam.Map(lambda element: (element['order_id']+','+element['product_id'] +','+element['seller_id']+','+element['shipping_limit_date'], float(element['price'])) ) \n",
    "                | 'Group sum prices' >> beam.CombinePerKey(sum)\n",
    "    )\n",
    "\n",
    "    itens_count = (\n",
    "            input_itens_orders \n",
    "                | 'dict for count itens' >> beam.Map(lambda element: (element['order_id']+','+element['product_id'] +','+element['seller_id']+','+element['shipping_limit_date'], int(element['order_item_id']))) \n",
    "                | 'count itens' >> beam.combiners.Count.PerKey()\n",
    "    )\n",
    "\n",
    "    freight_value = (\n",
    "            input_itens_orders \n",
    "                | 'dict for sum freight_value' >> beam.Map(lambda element: (element['order_id']+','+element['product_id'] +','+element['seller_id']+','+element['shipping_limit_date'], float(element['freight_value'])) ) \n",
    "                | 'Group sum freight_value' >> beam.CombinePerKey(sum)\n",
    "    )\n",
    "\n",
    "\n",
    "    join1 = ({'itens_count': itens_count, 'itens_sum_price': itens_sum_price} \n",
    "                | 'itens_count + itens_sum_price' >> beam.CoGroupByKey()                                                               \n",
    "    )\n",
    "\n",
    "\n",
    "    join2 = ({'join1': join1, 'freight_value': freight_value} \n",
    "                | 'join1 + freight_value' >> beam.CoGroupByKey()\n",
    "                | 'dict to list ' >> beam.Map(dict_toList) \n",
    "                | 'create dict join2 ' >> beam.Map(lambda element: (element[2], element))\n",
    "    )\n",
    "\n",
    "\n",
    "    sellers = (\n",
    "        p1\n",
    "            | 'Read seller data' >> beam.io.ReadFromText(input_files['input_seller'],skip_header_lines=1)\n",
    "            | 'Clean data seller' >> beam.Map(cleandata) \n",
    "            | 'return seller data except geo id' >> beam.Map(retSeller)\n",
    "            | 'create dict from seller' >> beam.Map(lambda sellers: (sellers[0], sellers))\n",
    "#            | 'Write seller data' >> beam.io.WriteToText('product',file_name_suffix='.csv') \n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    join3 = ({'join2': join2, 'sellers': sellers} \n",
    "                | 'join2 + sellers' >> beam.CoGroupByKey()\n",
    "                | 'dict to list sellers' >> beam.FlatMap(reTupless) \n",
    "                | 'create dict join3 ' >> beam.Map(lambda itens: (itens[1], itens))#verificar essa linha\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    products = (\n",
    "            p1\n",
    "                | 'Read prod data' >> beam.io.ReadFromText(input_files['input_products'],skip_header_lines=1)\n",
    "                | 'Clean data prod' >> beam.Map(cleandata) \n",
    "                | 'return prod data ' >> beam.Map(retProd)\n",
    "                | 'create dict from prod' >> beam.Map(lambda prod: (prod[0], prod))\n",
    "   )\n",
    "\n",
    "\n",
    "\n",
    "    join4 = ({'join3': join3, 'products': products} \n",
    "                | 'join3 + products' >> beam.CoGroupByKey()\n",
    "                | 'dict to list join4' >> beam.FlatMap(reTupless) \n",
    "                | 'Removing undisired caracters after join' >> beam.Map(ListtoStr_NN)\n",
    "                | 'Clean data and preparing list' >> beam.Map(cleandata) \n",
    "                | 'dict order id key join 4 ' >> beam.Map(lambda itens: (itens[0], itens))\n",
    "\n",
    "    )  \n",
    "\n",
    "\n",
    "        \n",
    "    order = (\n",
    "            p1\n",
    "                | 'Read orders data' >> beam.io.ReadFromText(input_files['input_order'],skip_header_lines=1)\n",
    "                | 'Clean data orders' >> beam.Map(cleandata) \n",
    "                | 'create dict from orders' >> beam.Map(lambda order: (order[0], order))\n",
    "    #            | 'Write seller data' >> beam.io.WriteToText('outputs/dept.txt') \n",
    "    )\n",
    "\n",
    "\n",
    "    reviews = (\n",
    "            p1\n",
    "                | 'Read reviews data' >> beam.io.ReadFromText(input_files['input_reviews'],skip_header_lines=1)\n",
    "                | 'Clean reviews order' >> beam.Map(cleandata) \n",
    "                | 'return order id and review score' >> beam.Map(retReview)\n",
    "                | 'Check reviews only with id' >> beam.Filter(lambda rev: rev is not None )\n",
    "                | 'create dict from reviews' >> beam.Map(lambda reviews: (reviews[0], reviews[1]))         \n",
    "    )\n",
    "\n",
    "    join5 = ({'order': order, 'reviews': reviews} \n",
    "                | 'orders + reviews' >> beam.CoGroupByKey()\n",
    "                | 'dict to list join5' >> beam.FlatMap(reTupless) \n",
    "                | 'create dict join 5 ' >> beam.Map(lambda itens: (itens[0], itens))\n",
    "    )\n",
    "\n",
    "   \n",
    "    payments = (\n",
    "        p1\n",
    "            | 'Read payments data' >> beam.io.ReadFromText(input_files['input_payments'],skip_header_lines=1)\n",
    "            | 'Clean payments order' >> beam.Map(cleandata) \n",
    "            | 'create dict from payments' >> beam.Map(lambda itens: (itens[0], itens))   \n",
    "    )\n",
    "\n",
    "    join6 = ({ 'payments': payments,'join5': join5} \n",
    "                | ' payments + join5 ' >> beam.CoGroupByKey()\n",
    "                | 'dict to list join6' >> beam.FlatMap(reTupless) \n",
    "                | 'create dict join 6' >> beam.Map(lambda itens: (itens[6], itens))\n",
    "    )\n",
    "    \n",
    "    customer = (\n",
    "        p1\n",
    "            | 'Read customer data' >> beam.io.ReadFromText(input_files['input_customer'],skip_header_lines=1)\n",
    "            | 'Clean customer order' >> beam.Map(cleandata) \n",
    "            | 'return customer id ,state and City' >> beam.Map(retCustomer)\n",
    "            | 'create dict from customer' >> beam.Map(lambda itens: (itens[0], itens))        \n",
    "#            | 'Write results customer' >> beam.io.WriteToText('customer',file_name_suffix='.csv')\n",
    "    )\n",
    "\n",
    "    join7 = ({ 'customer': customer, 'join6': join6} \n",
    "            | 'customer + join6' >> beam.CoGroupByKey()\n",
    "            | 'dict to list join7' >> beam.FlatMap(reTupless) \n",
    "            | 'dict order id key join 7  ' >> beam.Map(lambda itens: (itens[8], itens))\n",
    "    )\n",
    "\n",
    "\n",
    "    join8 = ({ 'join7': join7, 'join4': join4} \n",
    "             | 'join7 + join4' >> beam.CoGroupByKey()\n",
    "             | 'dict to list join8' >> beam.FlatMap(reTupless) \n",
    "             | 'selection field in desired order' >> beam.Map(returnFieldsJoin7Join4,current_date)  \n",
    "             | 'List to string' >> beam.Map(ListtoStr_NN)             \n",
    "             | 'Write results join 8' >> beam.io.WriteToText(input_files['output'], file_name_suffix='.csv',header=output_header)\n",
    "     )\n",
    "    \n",
    "    \n",
    "\n",
    "    # Executando o pipeline localmente\n",
    "    p1.run()\n",
    "\n",
    "\n",
    "#26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No files found based on the file pattern olist_order_items_dataset.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Definindo os parâmetros de entrada e saída diretamente\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_files \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_itens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124molist_order_items_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_seller\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124molist_sellers_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrun_pipeline_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 126\u001b[0m, in \u001b[0;36mrun_pipeline_local\u001b[0;34m(input_files)\u001b[0m\n\u001b[1;32m    117\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcampo_0,campo_1,campo_2,campo_3,campo_4,campo_5,campo_6,campo_7,campo_8,campo_9,campo_10,campo_11,campo_12,campo_13,campo_14,campo_15,campo_16,campo_17,campo_18,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124mcampo_19,campo_20,campo_21,campo_22,campo_23,campo_24,campo_25,campo_26,campo_27,campo_28\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    121\u001b[0m cols_itens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_item_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseller_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshipping_limit_date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreight_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    124\u001b[0m input_itens_orders \u001b[38;5;241m=\u001b[39m ( \n\u001b[1;32m    125\u001b[0m             p1 \n\u001b[0;32m--> 126\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRead itens_orders data\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m \u001b[43mbeam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReadFromText\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_itens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mskip_header_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClean data itens_orders\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(cleandata) \n\u001b[1;32m    128\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict for itens_orders\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(list_to_dict, cols_itens)    \n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    132\u001b[0m itens_sum_price \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m         input_itens_orders  \n\u001b[1;32m    134\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict for sum prices\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(\u001b[38;5;28;01mlambda\u001b[39;00m element: (element[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39melement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39melement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseller_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39melement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshipping_limit_date\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mfloat\u001b[39m(element[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m])) ) \n\u001b[1;32m    135\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroup sum prices\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mCombinePerKey(\u001b[38;5;28msum\u001b[39m)\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    138\u001b[0m itens_count \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m         input_itens_orders \n\u001b[1;32m    140\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict for count itens\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(\u001b[38;5;28;01mlambda\u001b[39;00m element: (element[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39melement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39melement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseller_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39melement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshipping_limit_date\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mint\u001b[39m(element[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_item_id\u001b[39m\u001b[38;5;124m'\u001b[39m]))) \n\u001b[1;32m    141\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount itens\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mcombiners\u001b[38;5;241m.\u001b[39mCount\u001b[38;5;241m.\u001b[39mPerKey()\n\u001b[1;32m    142\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/apache_beam/io/textio.py:782\u001b[0m, in \u001b[0;36mReadFromText.__init__\u001b[0;34m(self, file_pattern, min_bundle_size, compression_type, strip_trailing_newlines, coder, validate, skip_header_lines, delimiter, escapechar, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the :class:`ReadFromText` transform.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;124;03m    delimiter, can also escape itself.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_bundle_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrip_trailing_newlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_header_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_header_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/apache_beam/io/textio.py:141\u001b[0m, in \u001b[0;36m_TextSource.__init__\u001b[0;34m(self, file_pattern, min_bundle_size, compression_type, strip_trailing_newlines, coder, buffer_size, validate, skip_header_lines, header_processor_fns, delimiter, escapechar)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    107\u001b[0m     file_pattern,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a _TextSource\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  of the arguments.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmin_bundle_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompression_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_trailing_newlines \u001b[38;5;241m=\u001b[39m strip_trailing_newlines\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression_type \u001b[38;5;241m=\u001b[39m compression_type\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/apache_beam/io/filebasedsource.py:127\u001b[0m, in \u001b[0;36mFileBasedSource.__init__\u001b[0;34m(self, file_pattern, min_bundle_size, compression_type, splittable, validate)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_splittable \u001b[38;5;241m=\u001b[39m splittable\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mand\u001b[39;00m file_pattern\u001b[38;5;241m.\u001b[39mis_accessible():\n\u001b[0;32m--> 127\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/apache_beam/options/value_provider.py:193\u001b[0m, in \u001b[0;36mcheck_accessible.<locals>._check_accessible.<locals>._f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mis_accessible():\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mRuntimeValueProviderError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not accessible\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m obj)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfnc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/apache_beam/io/filebasedsource.py:189\u001b[0m, in \u001b[0;36mFileBasedSource._validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m match_result \u001b[38;5;241m=\u001b[39m FileSystems\u001b[38;5;241m.\u001b[39mmatch([pattern], limits\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(match_result\u001b[38;5;241m.\u001b[39mmetadata_list) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 189\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo files found based on the file pattern \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m pattern)\n",
      "\u001b[0;31mOSError\u001b[0m: No files found based on the file pattern olist_order_items_dataset.csv"
     ]
    }
   ],
   "source": [
    " # Definindo os parâmetros de entrada e saída diretamente\n",
    "input_files = {\n",
    "    'input_itens': 'olist_order_items_dataset.csv',\n",
    "    'input_seller': 'olist_sellers_dataset.csv',\n",
    "    'input_products': 'olist_products_dataset.csv',\n",
    "    'input_order': 'olist_orders_dataset.csv',\n",
    "    'input_reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'input_payments': 'olist_order_payments_dataset.csv',\n",
    "    'input_customer': 'olist_customers_dataset.csv',\n",
    "    'output': 'output'\n",
    "}\n",
    "run_pipeline_local(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
